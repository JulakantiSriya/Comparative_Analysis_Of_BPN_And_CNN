{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b8zfcCPlcDwC"
      },
      "outputs": [],
      "source": [
        "#import lib\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.datasets import cifar10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aNp9kgLc-T9",
        "outputId": "b4cf7f9f-1c72-4c26-8f10-62539c36b89a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#data set preparation\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#  cifar10.load_data() - function loads the CIFAR-10 dataset\n",
        "#  60,000 32x32 color images\n",
        "#  categorized into 10 classes (airplane, car, bird, etc.)\n",
        "#  80-20 Split (Common Practice)\n",
        "# 80% for training (50,000 images)-80% ensures the model has sufficient examples to learn effectively.\n",
        "# 20% for testing (10,000 images)-20% ensures reliable evaluation of the model without overloading the testing set.\n",
        "\n",
        "# Normalize the dataset\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "#Each pixel value in the images is initially an integer between 0 and 255 . Dividing by 255.0 normalizes these values to the range [0, 1]\n",
        "#Dividing by 255.0 normalizes these values to the range [0, 1], which improves numerical stability--- and ensures faster training.testing dataset.\n",
        "#Values are scaled to [0, 1]\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "def one_hot_encode(labels, num_classes=10):\n",
        "    one_hot = np.zeros((labels.size, num_classes))    #Creates a 2D array filled with zeros.\n",
        "    one_hot[np.arange(labels.size), labels.flatten()] = 1\n",
        "    return one_hot\n",
        "\n",
        "# This function converts the integer labels into a binary matrix format known as one-hot encoding.\n",
        "# Example: If num_classes=10 and the label is 2, the one-hot encoded vector is [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].\n",
        "# labels:Input array containing the class labels.\n",
        "# Total number of classes, default is 10 for CIFAR-10.\n",
        "# Returns a numpy array where each row is the one-hot encoded vector for the corresponding label.\n",
        "\n",
        "\n",
        "y_train = one_hot_encode(y_train)\n",
        "y_test = one_hot_encode(y_test)\n",
        "\n",
        "# Converts the integer labels in y_train and y_test into one-hot encoded matrices using the one_hot_encode function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lOnzfI8joYS6"
      },
      "outputs": [],
      "source": [
        "class BPN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights for input to hidden layer with small random values\n",
        "        self.w1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        # Initialize biases for hidden layer to zero\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        # Initialize weights for hidden to output layer with small random values\n",
        "        self.w2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        # Initialize biases for output layer to zero\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # Apply sigmoid activation function element-wise\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        # Calculate derivative of sigmoid activation function\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Store input for use in backward pass\n",
        "        self.X = X\n",
        "        # Calculate pre-activation values for the hidden layer\n",
        "        self.z1 = np.dot(X, self.w1) + self.b1\n",
        "        # Apply sigmoid activation to hidden layer\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        # Calculate pre-activation values for the output layer\n",
        "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        # Apply sigmoid activation to output layer\n",
        "        self.output = self.sigmoid(self.z2)\n",
        "        # Return the final output of the network\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, y):\n",
        "        # Calculate the error at the output layer\n",
        "        output_error = y - self.output\n",
        "        # Calculate the gradient of the error with respect to output activations\n",
        "        output_delta = output_error * self.sigmoid_derivative(self.output)\n",
        "        # Calculate the error propagated to the hidden layer\n",
        "        a1_error = np.dot(output_delta, self.w2.T)\n",
        "        # Calculate the gradient of the error with respect to hidden activations\n",
        "        a1_delta = a1_error * self.sigmoid_derivative(self.a1)\n",
        "\n",
        "        # Update weights between hidden and output layer using gradients\n",
        "        self.w2 += np.dot(self.a1.T, output_delta) * 0.01\n",
        "        # Update biases for the output layer using gradients\n",
        "        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * 0.01\n",
        "        # Update weights between input and hidden layer using gradients\n",
        "        self.w1 += np.dot(self.X.T, a1_delta) * 0.01\n",
        "        # Update biases for the hidden layer using gradients\n",
        "        self.b1 += np.sum(a1_delta, axis=0, keepdims=True) * 0.01\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):  # Iterate through the specified number of epochs\n",
        "            self.forward(X)          # Perform forward propagation for the input data\n",
        "            self.backward(y)         # Perform backward propagation to update weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vXmRnH4eutIz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class CNN:\n",
        "    def __init__(self, input_shape, num_classes, kernel_size=3, num_kernels=8, learning_rate=0.01):\n",
        "        # Initialize CNN parameters\n",
        "        self.input_shape = input_shape  # Shape of input images (height, width)\n",
        "        self.num_classes = num_classes  # Number of output classes\n",
        "        self.kernel_size = kernel_size  # Size of convolutional filters (e.g., 3x3)\n",
        "        self.num_kernels = num_kernels  # Number of convolutional filters\n",
        "        self.learning_rate = learning_rate  # Learning rate for gradient updates\n",
        "\n",
        "        # Initialize convolution kernels with random values\n",
        "        self.kernels = np.random.rand(num_kernels, kernel_size, kernel_size) - 0.5\n",
        "        # Calculate the size of the flattened convolution output\n",
        "        flattened_size = num_kernels * ((input_shape[0] - kernel_size + 1) *\n",
        "                                        (input_shape[1] - kernel_size + 1))\n",
        "        # Initialize weights for fully connected layer\n",
        "        self.weights = np.random.rand(flattened_size, num_classes) - 0.5\n",
        "        # Initialize biases for the fully connected layer\n",
        "        self.bias = np.zeros((1, num_classes))\n",
        "\n",
        "    def relu(self, x):\n",
        "        # Apply ReLU activation function\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        # Compute derivative of ReLU for backpropagation\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Apply softmax activation function for output probabilities\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stabilize with max subtraction\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # Normalize to probabilities\n",
        "\n",
        "    def convolve(self, x, kernel):\n",
        "        # Perform valid convolution on the input image with a kernel\n",
        "        output_dim = x.shape[0] - kernel.shape[0] + 1  # Calculate output size\n",
        "        convolved = np.zeros((output_dim, output_dim))  # Initialize output matrix\n",
        "        for i in range(output_dim):  # Slide kernel over rows\n",
        "            for j in range(output_dim):  # Slide kernel over columns\n",
        "                convolved[i, j] = np.sum(x[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel)\n",
        "                # Compute the sum of element-wise multiplication\n",
        "        return convolved\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        # Apply convolution with all kernels\n",
        "        self.convolved_outputs = np.array([self.convolve(x[0], kernel) for kernel in self.kernels])\n",
        "        self.feature_map_shape = self.convolved_outputs.shape  # Save shape for backpropagation\n",
        "\n",
        "        # Apply ReLU activation\n",
        "        self.convolved_outputs = self.relu(self.convolved_outputs)\n",
        "\n",
        "        # Flatten feature maps for the fully connected layer\n",
        "        fc_input = self.convolved_outputs.flatten().reshape(1, -1)\n",
        "\n",
        "        # Compute class scores and probabilities using softmax\n",
        "        self.output = self.softmax(np.dot(fc_input, self.weights) + self.bias)\n",
        "\n",
        "        return self.output  # Return predicted probabilities\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        # Compute gradients and update weights\n",
        "        m = y.shape[0]  # Number of samples in the batch\n",
        "\n",
        "        # Compute error at the output layer\n",
        "        output_error = self.output - y\n",
        "\n",
        "        # Compute gradients for the fully connected layer weights and biases\n",
        "        fc_input = self.convolved_outputs.flatten().reshape(1, -1)\n",
        "        self.fc_weights_gradient = np.dot(fc_input.T, output_error) / m  # Gradient for weights\n",
        "        self.fc_bias_gradient = np.sum(output_error, axis=0, keepdims=True) / m  # Gradient for biases\n",
        "\n",
        "        # Propagate error back to the feature maps\n",
        "        flattened_error = np.dot(output_error, self.weights.T)  # Backpropagate to the flattened layer\n",
        "        feature_map_error = flattened_error.reshape(self.feature_map_shape)  # Reshape to feature map dimensions\n",
        "\n",
        "        # Apply ReLU derivative to feature map error\n",
        "        relu_error = feature_map_error * self.relu_derivative(self.convolved_outputs)\n",
        "\n",
        "        # Compute gradients for convolutional kernels\n",
        "        self.conv_weights_gradient = np.zeros_like(self.kernels)\n",
        "        input_image = x[0]  # Assuming a single input image\n",
        "        for i, kernel in enumerate(self.kernels):  # Iterate over each kernel\n",
        "            for j in range(relu_error.shape[1]):  # Iterate over rows of error map\n",
        "                for k in range(relu_error.shape[2]):  # Iterate over columns of error map\n",
        "                    # Update gradient for the current kernel\n",
        "                    self.conv_weights_gradient[i] += (\n",
        "                        relu_error[i, j, k] * input_image[j:j+self.kernel_size, k:k+self.kernel_size]\n",
        "                    )\n",
        "\n",
        "        # Normalize convolution kernel gradients\n",
        "        self.conv_weights_gradient /= m\n",
        "\n",
        "        # Update weights and biases with gradients\n",
        "        self.weights -= self.learning_rate * self.fc_weights_gradient\n",
        "        self.bias -= self.learning_rate * self.fc_bias_gradient\n",
        "        self.kernels -= self.learning_rate * self.conv_weights_gradient\n",
        "\n",
        "    def train(self, x, y, epochs=10):\n",
        "        # Train the CNN for the specified number of epochs\n",
        "        for epoch in range(epochs):  # Iterate over epochs\n",
        "            output = self.forward(x)  # Perform forward pass\n",
        "            loss = -np.sum(y * np.log(output)) / y.shape[0]  # Compute cross-entropy loss\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")  # Print loss for monitoring\n",
        "            self.backward(x, y)  # Perform backward pass and update weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8s8HWhFipChs"
      },
      "outputs": [],
      "source": [
        "# **Prepare data for BPN**\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        " # Flatten the training images into 1D arrays\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        " # Flatten the testing images into 1D arrays\n",
        "\n",
        "# **Train BPN**\n",
        "bpn = BPN(input_size=x_train_flat.shape[1], hidden_size=64, output_size=10)\n",
        " # Initialize BPN with input size, hidden size, and output size\n",
        "start_time = time.time()\n",
        " # Record start time to measure training duration\n",
        "bpn.train(x_train_flat[:1000], y_train[:1000], epochs=10)\n",
        "# Train the BPN using the first 1000 training samples for 10 epochs\n",
        "bpn_time = time.time() - start_time\n",
        " # Calculate the total time taken for BPN training\n",
        "\n",
        "# **Train CNN**\n",
        "cnn = CNN(input_shape=x_train[0].shape, num_classes=10)\n",
        " # Initialize CNN with input image shape and number of classes\n",
        "start_time = time.time()\n",
        "# Record start time to measure training duration\n",
        "for i in range(1000):\n",
        " # Loop through the first 1000 training samples\n",
        "    cnn.forward(x_train[i:i+1])\n",
        "     # Perform forward pass for each sample (training step can be extended)\n",
        "cnn_time = time.time() - start_time\n",
        " # Calculate the total time taken for CNN forward passes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfaMVplgq7Rg",
        "outputId": "1b242f0a-3189-43d1-fe66-8d00576bd434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BPN Training Time: 0.38 seconds\n",
            "CNN Training Time: 96.84 seconds\n"
          ]
        }
      ],
      "source": [
        "print(f\"BPN Training Time: {bpn_time:.2f} seconds\")\n",
        "print(f\"CNN Training Time: {cnn_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_CQroI5pknl",
        "outputId": "dafe1758-a8fc-4064-e585-90520ce17a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BPN Accuracy: 10.90%\n",
            "CNN Accuracy: 12.70%\n"
          ]
        }
      ],
      "source": [
        "# **Accuracy Calculation**\n",
        "def calculate_accuracy(predictions, labels):\n",
        "  # Function to calculate accuracy\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    # Get class with the highest probability from predictions\n",
        "    true_classes = np.argmax(labels, axis=1)\n",
        "     # Get the actual class from labels\n",
        "    accuracy = np.mean(predicted_classes == true_classes) * 100\n",
        "     # Calculate the percentage of correct predictions\n",
        "    return accuracy\n",
        "     # Return the computed accuracy\n",
        "\n",
        "# **BPN Accuracy**\n",
        "bpn_predictions = bpn.forward(x_test_flat[:1000])\n",
        "# Get BPN predictions for the first 1000 test samples\n",
        "bpn_accuracy = calculate_accuracy(bpn_predictions, y_test[:1000])\n",
        " # Calculate accuracy for BPN predictions\n",
        "print(f\"BPN Accuracy: {bpn_accuracy:.2f}%\")\n",
        " # Print the BPN accuracy percentage\n",
        "\n",
        "# **CNN Accuracy**\n",
        "cnn_predictions = []\n",
        " # Initialize a list to store CNN predictions\n",
        "for i in range(1000):\n",
        "   # Loop through the first 1000 test samples\n",
        "    cnn_predictions.append(cnn.forward(x_test[i:i+1]))\n",
        "     # Get CNN predictions for each test sample\n",
        "cnn_predictions = np.vstack(cnn_predictions)\n",
        " # Combine all predictions into a single array\n",
        "cnn_accuracy = calculate_accuracy(cnn_predictions, y_test[:1000])\n",
        " # Calculate accuracy for CNN predictions\n",
        "print(f\"CNN Accuracy: {cnn_accuracy:.2f}%\")\n",
        "# Print the CNN accuracy percentage\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
